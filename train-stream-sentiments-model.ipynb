{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f1da51",
   "metadata": {},
   "source": [
    "<h1>Stream Sentiments</h1>\n",
    "<h3>Goals for this project:</h3>\n",
    "<ul><li>Train a neural network to predict whether content creators on Twitch.tv are receiving positive or negative comments from members of their chat.</li><li>Deploy this network on Google Cloud AI Platform.</li><li>Create a website (<a href="https://streamsentiments.com/">here</a>) that receives live chat messages from the Twitch API, and generates metrics from them. From this, we will be able to judge the overall sentiment that viewers feel about the content creators they watch.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5f0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af2854",
   "metadata": {},
   "source": [
    "<h1>Get the data</h1>\n",
    "<h3>Notes:</h3>\n",
    "<ul><li>Since the data coming from live Twitch chat is inherently unlabeled, we need to train our model on another dataset.</li><li>Ideally, our training set should be a dataset of Twitch chat messages along with their sentiment labels. However, not only could I not find such a labeled dataset, but such a set of short-form messages would intractable by me, since my hardware forces me to use pre-trained word embeddings (which we will see later).</li><li>These pre-trained embeddings obviously don't contain the often-misspelled and colloquial messages found in Twitch chat, which is unfortunate. In fact, the GloVe word embedding that I tried (see below) only contained about 20% of the words in a Twitter dataset that I tried using for this project. This would lead to many out-of-vocabulary tokens, a long time for training to converge, and noisy results. Moreover, the incredible variation in the social-media-style messages found on Twitch would be unlikely to be covered by any word-level embedding scheme.</li><li>The following dataset of Amazon reviews (<a href=\"https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews?resource=download\">here</a>) should be a good fit. It's labeled, and it should not produce many out-of-vocabulary tokens in our word embeddings.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e88786",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This soundtrack is my favorite music of all ti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you've played the game, you know how divine...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am quite sure any of you actually taking the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  I'm reading a lot of reviews saying that this ...          2\n",
       "1  This soundtrack is my favorite music of all ti...          2\n",
       "2  I truly like this soundtrack and I enjoy video...          2\n",
       "3  If you've played the game, you know how divine...          2\n",
       "4  I am quite sure any of you actually taking the...          2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/train.csv\", encoding='utf-8')\n",
    "df = df.iloc[:, [2, 0]]\n",
    "df.columns = ['text', 'sentiment'] # sentiment = 2 is positive, sentiment = 1 is negative\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa995f84",
   "metadata": {},
   "source": [
    "<ul><li>Let's see if these comments are true to their labels.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2132891c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\",\n",
       " 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0], df.sentiment[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d96a3",
   "metadata": {},
   "source": [
    "<ul><li>Seems pretty positive to me, so this looks good.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9c7b08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This is a self-published book, and if you want to know why--read a few paragraphs! Those 5 star reviews must have been written by Ms. Haddon\\'s family and friends--or perhaps, by herself! I can\\'t imagine anyone reading the whole thing--I spent an evening with the book and a friend and we were in hysterics reading bits and pieces of it to one another. It is most definitely bad enough to be entered into some kind of a \"worst book\" contest. I can\\'t believe Amazon even sells this kind of thing. Maybe I can offer them my 8th grade term paper on \"To Kill a Mockingbird\"--a book I am quite sure Ms. Haddon never heard of. Anyway, unless you are in a mood to send a book to someone as a joke---stay far, far away from this one!',\n",
       " 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[5], df.sentiment[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522d3c2",
   "metadata": {},
   "source": [
    "<ul><li>That sure is negative!</li><li>It's also a good idea to look at how many positive reviews there are compared to negative reviews.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe00fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1799999, 1800000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.sentiment == 2]), len(df[df.sentiment == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b90bc3",
   "metadata": {},
   "source": [
    "<ul><li>We have an unskewed dataset, which means our trained model will learn equally well to recognize both positive and negative reviews.</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18970e5e",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>\n",
    "<h3>Notes:</h3>\n",
    "<ul><li>Split the dataset into a training set and a test set.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5d9f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3599999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = len(df)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb17d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3239999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_train_test_ratio = 0.9\n",
    "\n",
    "train_size = int(dataset_size * desired_train_test_ratio)\n",
    "train_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553538c",
   "metadata": {},
   "source": [
    "<ul><li>First of all, we should adjust positive sentiments to have a value of 1 and negative sentiments to have a value of 0.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05692236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sentiment -= 1 # easy enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409312b",
   "metadata": {},
   "source": [
    "<ul><li>Shuffle the dataset.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feac52b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"This book is nothing like Twilight. I found it on a booklist for Twilight read-alikes, and I understand why it was included, I'm just saying - it's not like Twilight. In fact, if you didn't like Twilight, you may like this book. The protagonist is as full of self-confidance as Bella is full of self-doubt. Whereas Bella's relationship with her family is peripheral, Viviane is surrounded by family. She's part of a pack after all. She's a werewolf. In the end Viviane's dilemna is not far from Bella's; she wishes she could have a friend outside the pack. The theme is a perennial favorite with young adult fiction - finding where you fit in outside the stifling structure of one's family. I didn't like this one because it's too transparent. Too obvious. The angst is spelled out too clearly.\",\n",
       "       0], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.to_numpy()\n",
    "np.random.shuffle(dataset)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e9c5d",
   "metadata": {},
   "source": [
    "<ul><li>Separate instances and labels.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3584cf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = dataset[:, 0] # instances\n",
    "y = dataset[:, 1] # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6ad7e",
   "metadata": {},
   "source": [
    "<ul><li>Create training and testing sets.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4d0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[:train_size], y[:train_size], \n",
    "X_valid, y_valid = X[train_size:], y[train_size:] # train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07785c7e",
   "metadata": {},
   "source": [
    "<h1>Create the Model</h1>\n",
    "<h3>Notes:</h3>\n",
    "<ul><li>We need to train a binary classifier that predicts whether an input sentence has a positive or negative sentiment.</li><li>The model will analyze the relationships between the words in the input (using an attention mechanism), and will process this information through a recurrent layer.</li><li>But first, we must preprocess the inputs by splitting the input text on whitespaces, then tokenizing the lists of words. This means we read in every unique word from all of the Amazon reviews and assign each of them to an integer, so that our neural network will be able to perform calculations on them.</li><li> Then we must pad the lists of words to the same length (with padding tokens that will be masked out when the model is evaluated). We will use the keras.layers.TextVectorization layer to accomplish all of these tasks.</li></ul></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aedf3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=[], dtype=tf.string) \n",
    "\n",
    "# split, tokenize, pad \n",
    "tokenizer = keras.layers.TextVectorization() # padding token is 0, OOV token is 1\n",
    "tokenizer.adapt(X) # fit a vocabulary of words onto our dataset\n",
    "vocab_size = len(tokenizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11e876ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a smaller vocabulary for the model to be able to be saved as a SavedModel\n",
    "truncated_vocab_size = vocab_size // 2\n",
    "input_ = keras.layers.Input(shape=[], dtype=tf.string) \n",
    "\n",
    "# split, tokenize, pad \n",
    "tokenizer = keras.layers.TextVectorization(max_tokens=truncated_vocab_size) # padding token is 0, OOV token is 1\n",
    "tokenizer.adapt(X) # fit a vocabulary of words onto our dataset\n",
    "vocab_size = len(tokenizer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b37042",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = tokenizer(input_) # keras Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf8a14",
   "metadata": {},
   "source": [
    "<ul><li>Now, we will embed our tokens into a higher-dimensional space. Such embedding layers typically require lots of parameters to train, so we will use pre-trained GloVe embeddings provided by Stanford. These assign vector representations to each word, such that vectors corresponding to semantically similar words are close together, and that the axes of the embedding space carry semantic meaning as well.</li><li>Specifically, we will use GloVe embeddings that have been trained on Twitter data. These embeddings will be 25-dimensional, and we need to make a matrix that assigns the appropriate embedding to the token for each word in our vocabulary.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6b76da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('user',\n",
       " [1.3684,\n",
       "  -0.0175,\n",
       "  0.18439,\n",
       "  -0.099826,\n",
       "  1.8532,\n",
       "  -1.1886,\n",
       "  0.17059,\n",
       "  0.42307,\n",
       "  0.6154,\n",
       "  -0.55661,\n",
       "  -0.1908,\n",
       "  -1.1043,\n",
       "  -2.6535,\n",
       "  -1.4463,\n",
       "  -0.14067,\n",
       "  -0.12293,\n",
       "  -0.33451,\n",
       "  0.098061,\n",
       "  -0.25334,\n",
       "  -0.77808,\n",
       "  -0.15077,\n",
       "  -1.1084,\n",
       "  -0.15894,\n",
       "  0.89311,\n",
       "  -1.4058])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed using GloVe\n",
    "\n",
    "# example line from the downloaded GloVe file:\n",
    "\n",
    "# <url> 0.80384 -1.0366 -0.53877 -1.0806 0.84718 -0.36196 1.0065 1.3067 -0.61225 0.30781 0.46974 \\\n",
    "# -0.23264 -3.3882 -0.46778 -0.55105 -1.6926 -0.78708 0.28378 -0.73638 0.10216 -0.18703 -2.133 \\\n",
    "# -0.17787 -0.97788 1.394\n",
    "\n",
    "embed_size = 25\n",
    "embedding_dict = {}\n",
    "with open(f\"./glove/glove.twitter.27B.{embed_size}d.txt\", encoding=\"utf8\") as fh:\n",
    "    for line in fh.readlines():\n",
    "        split = line.split()\n",
    "        word = split[0].lstrip(\"<\").rstrip(\">\")\n",
    "        embedding = [ float(value) for value in split[1:] ]\n",
    "        embedding_dict.update({word : embedding})\n",
    "    \n",
    "list(embedding_dict.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f62515a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8634238522460367"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros(shape=(truncated_vocab_size, embed_size))\n",
    "\n",
    "not_found = [] # keep track of which words couldn't be encoded\n",
    "for (token, word) in enumerate(tokenizer.get_vocabulary()):\n",
    "    if word in embedding_dict.keys():\n",
    "        embedding_matrix[token] = embedding_dict[word]\n",
    "    else:\n",
    "        embedding_matrix[token] = np.random.normal(size=(embed_size,))# randomly initialize the embedding of tokens that could not be found\n",
    "        # since we initialize these with the standard normal distribution, they (which includes the OOV token) will be close together in embedding space, so the model will collectively recognize them as OOVs\n",
    "        not_found.append(word)\n",
    "        \n",
    "len(not_found) / truncated_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36afc6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " '30',\n",
       " '14',\n",
       " '16',\n",
       " '60s',\n",
       " 'ps2',\n",
       " '400',\n",
       " '360',\n",
       " '1970s',\n",
       " '911',\n",
       " '120',\n",
       " 'fastpaced',\n",
       " 'hisher',\n",
       " '350',\n",
       " '46',\n",
       " '43',\n",
       " 'characterizations',\n",
       " '1015',\n",
       " '1989',\n",
       " 'ww2',\n",
       " '1987',\n",
       " 'moviethe',\n",
       " '1969',\n",
       " '199',\n",
       " '1968',\n",
       " 'etci',\n",
       " 'firstrate',\n",
       " '128',\n",
       " '3x',\n",
       " '62',\n",
       " '2gb',\n",
       " '94',\n",
       " '79',\n",
       " 'singersongwriter',\n",
       " '299',\n",
       " 'bookbut',\n",
       " '1966',\n",
       " '08',\n",
       " '1100',\n",
       " '4gb',\n",
       " 'ps1',\n",
       " 'tongueincheek',\n",
       " 'firstperson',\n",
       " '710',\n",
       " 'lifethe',\n",
       " '170',\n",
       " '91',\n",
       " 'wrongi',\n",
       " 'agoi',\n",
       " 'boringi',\n",
       " 'coauthor',\n",
       " '1080i',\n",
       " 'mitford',\n",
       " 'wellrounded',\n",
       " 'arthurian',\n",
       " 'runofthemill',\n",
       " 'wellit',\n",
       " 'b4',\n",
       " '1955',\n",
       " '09',\n",
       " '799',\n",
       " 'receipes',\n",
       " '50000',\n",
       " 'mebut',\n",
       " '1939',\n",
       " '5050',\n",
       " 'oneif',\n",
       " 'oneand',\n",
       " 'krakauer',\n",
       " '256mb',\n",
       " '8gb',\n",
       " '1954',\n",
       " '103',\n",
       " 'itafter',\n",
       " '4x4',\n",
       " 'worksi',\n",
       " 'easytofollow',\n",
       " 'koontzs',\n",
       " 'blink182',\n",
       " 'overi',\n",
       " 'xbox360',\n",
       " 'workthis',\n",
       " 'poignancy',\n",
       " 'audiovideo',\n",
       " '1299',\n",
       " 'betterthis',\n",
       " 'wellbuilt',\n",
       " 'charactersthe',\n",
       " 'sturdiness',\n",
       " 'nonchristian',\n",
       " 'problems1',\n",
       " 'wellbalanced',\n",
       " '2the',\n",
       " 'mid80s',\n",
       " '40000',\n",
       " 'compatability',\n",
       " 'throughi',\n",
       " 'backordered',\n",
       " 'greatthis',\n",
       " 'albumand']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found[:1000:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66bc5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed \n",
    "embed = keras.layers.Embedding(vocab_size,\n",
    "                               embed_size, \n",
    "                               embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                               mask_zero=True, # ignore padding tokens\n",
    "                               trainable=False)(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b720ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del embedding_dict # free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc61dae",
   "metadata": {},
   "source": [
    "<ul><li>Now we will apply a positional embedding. This is because an attention mechanism doesn't inherently know the relative or absolute positions of words in the input sentence, so we will encode this information by simply adding the token indices to the tokens themselves.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c571db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional embed (add word indices along the time dimension)\n",
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "    def call(self, inputs):\n",
    "        time_series_length = tf.shape(inputs)[1]\n",
    "        return inputs + tf.range(time_series_length, dtype=self.dtype)[tf.newaxis, :, tf.newaxis]\n",
    "\n",
    "positional_embed = PositionalEmbedding()(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cf1f7",
   "metadata": {},
   "source": [
    "<ul><li>Next, we will use a self-attention layer. It calculates the relationships between the words in the input sentence, and performs linear combinations of their embeddings according to the strength of these relationships. The use_scale argument enables the layer to scale the relationships between the words by a learned parameter.</li><li>Finally, we will add on a dense layer, since the attention layer only has one learnable parameter and we need more parameters to tweak during training.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508200fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "attention = keras.layers.Attention(use_scale=True, dropout=0.2)([positional_embed, positional_embed])\n",
    "dense = keras.layers.Dense(25, activation=\"elu\")(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be49cc",
   "metadata": {},
   "source": [
    "<ul><li>Now we will use a GRU layer. This is a recurrent layer, meaning that it processes each element of an input sequence consecutively, keeping a memory of what it has seen. We will only use the output of its final time step in computing the output of the model.</li><li>The output of the model will be fed into a sigmoid activation, since we are performing a binary classification task and we need a value between 0 and 1 to represent the probability of a positive sentiment.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85b4e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru1 = keras.layers.GRU(15, return_sequences=True, dropout=0.2, \n",
    "                        recurrent_dropout=0.2, activation=\"elu\")(dense)\n",
    "gru2 = keras.layers.GRU(10, return_sequences=False, dropout=0.2, \n",
    "                        recurrent_dropout=0.2, activation=\"elu\")(gru1)\n",
    "output_ = keras.layers.Dense(1, activation=\"sigmoid\")(gru2)\n",
    "\n",
    "# final model\n",
    "model = keras.Model(inputs=[input_], outputs=[output_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff2802",
   "metadata": {},
   "source": [
    "<h1>Train the Model</h1>\n",
    "<h3>Notes:</h3>\n",
    "<ul><li>First, let's look at how many parameters are in the model.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31c30783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_1 (TextVect  (None, None)        0           ['input_2[0][0]']                \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 25)     28046625    ['text_vectorization_1[0][0]']   \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 25)    0           ['embedding[0][0]']              \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, None, 25)     1           ['positional_embedding[0][0]',   \n",
      "                                                                  'positional_embedding[0][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 25)     650         ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, None, 15)     1890        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 10)           810         ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            11          ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,049,987\n",
      "Trainable params: 3,362\n",
      "Non-trainable params: 28,046,625\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b16dcf",
   "metadata": {},
   "source": [
    "<ul><li>It will be good for us to convert our training set to a tf.data Dataset, since Tensorflow is likely optimized for this data type.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f47da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, \n",
    "                                            y_train.astype(np.uint32)\n",
    "                                           )).shuffle(10000).batch(batch_size).prefetch(1)\n",
    "\n",
    "valid = tf.data.Dataset.from_tensor_slices((X_valid, \n",
    "                                            y_valid.astype(np.uint32)\n",
    "                                           )).shuffle(10000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f934bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0564581",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_set = train.take(1) # for testing purposes, only use one instance\n",
    "valid_set = valid.take(1)\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "109fa865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step - loss: 3.1535 - accuracy: 0.7500 - val_loss: 2.3168 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_set, epochs=epochs, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8501d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: stream_sentiments_model\\0005\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: stream_sentiments_model\\0005\\assets\n"
     ]
    }
   ],
   "source": [
    "model_version=\"0005\"\n",
    "model_name = \"stream_sentiments_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "\n",
    "model.save(model_path) # create the SavedModel using this function, so that it is saved with the keras metadata and we can load the keras model from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc6a135a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39742b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.577939"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the model\n",
    "\n",
    "message = \"Pleased to meet you\"\n",
    "\n",
    "model(tf.constant([message])).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bface",
   "metadata": {},
   "source": [
    "<ul><li>Now, we need to do a full-blown round of training. I tried this before on my laptop, and it ran for about 8 hours only to yield a training accuracy of 55%. This is understandable, since we are training our model on a fairly complex natural language task.</li><li>To train for longer, I need to move the training over to Google Cloud AI Platform. To speed up training, I will use the MultiWorkerMirroredStrategy class from TensorFlow's Distribution Strategies API. This will distribute the training instances across all available GPUs (data parallelism), run the training loop on each, and average the resulting set of parameter gradients in order to perform a gradient descent step.</li><li>Here is the training job I issued:</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams                                                                                                                                                                                       \n",
    "version = \"0005\" # model version to train\n",
    "epochs=10\n",
    "sample_perc = .5 # GCloud VMs have limited memory, so we can only use a fraction of the training set\n",
    "batch_size = 20\n",
    "continue_training_from_checkpoint=False\n",
    "\n",
    "# imports                                                                                                                                                                                       \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import pathlib\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--job-dir\", required=True, type=str) # required by GCloud\n",
    "args = parser.parse_args()\n",
    "\n",
    "# setup                                                                                                                                                                                         \n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(\"stream-sentiments\")\n",
    "\n",
    "distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy() # speed up training\n",
    "with distribution.scope():\n",
    "    \n",
    "    # load and preprocess data                                                                                                                                                                  \n",
    "    bucket.blob(\"train.csv\").download_to_filename(\"train.csv\")\n",
    "    train = pd.read_csv(\"train.csv\", encoding='utf-8').iloc[:, [2, 0]].sample(frac=sample_perc)\n",
    "    train.columns = ['text', 'sentiment']\n",
    "    train.sentiment -= 1\n",
    "    train = train.to_numpy()\n",
    "    X_train = train[:, 0]\n",
    "    y_train = train[:, 1]\n",
    "    \n",
    "    train = tf.data.Dataset.from_tensor_slices((X_train, y_train.astype(np.uint32))).shuffle(\n",
    "                                                                10000).batch(batch_size).prefetch(1)\n",
    "\n",
    "    # load the model                                                                                                                                                                            \n",
    "    base_path = \"stream_sentiments_model/\"+version+\"/\"\n",
    "    for blob in bucket.list_blobs(prefix=base_path):\n",
    "        parent_path = pathlib.Path(blob.name).parent\n",
    "        os.makedirs(parent_path.as_posix(), exist_ok=True)\n",
    "        blob.download_to_filename((parent_path / blob.name.split(\"/\")[-1]).as_posix())\n",
    "\n",
    "    model = keras.models.load_model(base_path)\n",
    "\n",
    "# train the model                                                                                                                                                                               \n",
    "if continue_training_from_checkpoint:\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    checkpoint_path = \"gs://stream-sentiments/checkpoints/\"\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "    \n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\"gs://stream-sentiments/\")\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath = \"gs://stream-sentiments/checkpoints/\"+version+\".ckpnt\",\n",
    "                                                      save_weights_only=True,\n",
    "                                                      verbose=1)\n",
    "history=model.fit(train, epochs=epochs, callbacks=[tensorboard_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a79b5",
   "metadata": {},
   "source": [
    "<ul><li>The training ran for 47 hours, and yielded an accuracy of 50% on the training set. This is no better than simply always guessing the sentiments to be positive (or negative), which is the absolute worst performance a binary classifier could have. The accuracy had reached 56% at one point, but gradient descent started diverging after.</li><li>It seems that the patterns in the training set are too complex for our model to learn. The obvious solution is to increase the number of neurons and layers in our model, but Google Cloud AI Platform has a limit on the file size of served models, which is 524,288,000 bytes. My larger model was nearly double the size: 944,409,388 bytes. With our GloVe embedding matrix saved with the model, we are bound to be close to this maximum size in any case.</li><li>Let's download the model checkpoint from GCloud and check our performance.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f97e78a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.InitializationOnlyStatus at 0x262a45d7820>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint_path = \"./checkpoints\" # downloaded from GCP\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f010f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.577939"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the model again\n",
    "\n",
    "message = \"Pleased to meet you\"\n",
    "\n",
    "model(tf.constant([message])).numpy()[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
