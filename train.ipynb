{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e77214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tqdm\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e88786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "df = pd.read_csv(\"./training.1600000.processed.noemoticon.csv\", encoding='ansi')\n",
    "df = df[['''@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D''', '0']]\n",
    "df.columns = ['tweet', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5d9f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = len(df.tweet)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb17d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1519999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(dataset_size * 0.95)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58068a5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690956"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenizer = Tokenizer(char_level=False, oov_token=0)\n",
    "tokenizer.fit_on_texts(df.tweet)\n",
    "vocab_size = len(tokenizer.word_index.keys())\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83df603d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized = tokenizer.texts_to_sequences(df.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c491d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get maximum tweet length\n",
    "max_tweet_length = max(len(sequence) for sequence in tokenized)\n",
    "max_tweet_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740d5983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\",\n",
       "        0],\n",
       "       ['@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds',\n",
       "        0],\n",
       "       ['my whole body feels itchy and like its on fire ', 0],\n",
       "       ...,\n",
       "       ['Are you ready for your MoJo Makeover? Ask me for details ', 4],\n",
       "       ['Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur ',\n",
       "        4],\n",
       "       ['happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H ',\n",
       "        4]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.to_numpy()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d47f642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Ok so after days of working on this giant jawbreaker, it turns out the center is made of HOLLOW bubblegum... I feel so gyped ',\n",
       "        0],\n",
       "       ['@JenniferSmithCo Just checked the diary and it clashes with uSwitch.net beers ',\n",
       "        0],\n",
       "       [\"Okay you know somethin is good when you dream about it. Land of the lost dreams are even stranger than the show. Can't wait for the movie \",\n",
       "        4],\n",
       "       ...,\n",
       "       [\"@CyberWasteland Last day o' the year, someone ran into a window and it fell on a teacher two floors down. \",\n",
       "        4],\n",
       "       ['ok i completely dont understand why people behave like shit to each other! ',\n",
       "        0],\n",
       "       ['@thedilettante but soon you will know the joy of crowding out namesakes on page 1 of search engines ',\n",
       "        4]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3584cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Ok so after days of working on this giant jawbreaker, it turns out the center is made of HOLLOW bubblegum... I feel so gyped ',\n",
       "        '@JenniferSmithCo Just checked the diary and it clashes with uSwitch.net beers ',\n",
       "        \"Okay you know somethin is good when you dream about it. Land of the lost dreams are even stranger than the show. Can't wait for the movie \",\n",
       "        ...,\n",
       "        \"@CyberWasteland Last day o' the year, someone ran into a window and it fell on a teacher two floors down. \",\n",
       "        'ok i completely dont understand why people behave like shit to each other! ',\n",
       "        '@thedilettante but soon you will know the joy of crowding out namesakes on page 1 of search engines '],\n",
       "       dtype=object),\n",
       " array([0, 0, 4, ..., 4, 0, 4], dtype=object))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to tf.Tensor\n",
    "X = dataset[:, 0]\n",
    "y = dataset[:, 1]\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4d0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / valid split\n",
    "X_train, y_train, X_valid, y_valid = X[:train_size], y[:train_size], X[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b32212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_allowed_length = 200\n",
    "\n",
    "# # preprocessing function for raw data\n",
    "# def preprocess(tweets, sentiments):\n",
    "#     tokenized = tokenizer.texts_to_sequences(list(tweets)) # tokenize\n",
    "#     ragged_tensor = tf.ragged.constant(tokenized) # ragged tensor\n",
    "#     dense_tensor = ragged_tensor.to_tensor(default_value=-1) # apply_padding\n",
    "#     if tf.shape(dense_tensor)[1] > max_allowed_length:\n",
    "#         dense_tensor = dense_tensor[:, :max_allowed_length] # truncate if needed\n",
    "#     X = dense_tensor + 1 # shift padding token to index 0\n",
    "#     y = (sentiments / 4).astype(np.uint32)\n",
    "#     return tf.constant(X, dtype=tf.int32), tf.constant(y, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e54f069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train_preprocessed, y_train_preprocessed = preprocess(X_train, y_train)\n",
    "# X_valid_preprocessed, y_valid_preprocessed = preprocess(X_valid, y_valid)\n",
    "# X_train_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a34579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 20\n",
    "\n",
    "# train = tf.data.Dataset.from_tensor_slices((X_train_preprocessed, y_train_preprocessed))\n",
    "\n",
    "# valid = tf.data.Dataset.from_tensor_slices((X_valid_preprocessed, y_valid_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "855e58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset to TFRecord file:\n",
    "\n",
    "# from tensorflow.train import Int64List, Feature, BytesList, Example, Features\n",
    "\n",
    "# def serialize_example(instance, label):\n",
    "#     X = tf.io.serialize_tensor(instance)\n",
    "#     feature = {\n",
    "#       'image': Feature(bytes_list=BytesList(value=[X.numpy()])),\n",
    "#       'label': Feature(int64_list=Int64List(value=[label.numpy()]))\n",
    "#     }\n",
    "#     example = Example(features=Features(feature=feature))\n",
    "#     return example.SerializeToString()\n",
    "\n",
    "# os.chdir(\"./datasets/\")\n",
    "# for j, dataset in enumerate([train, valid]):\n",
    "#     os.chdir(f\"./{j}/\")\n",
    "#     for i, inst in dataset.enumerate():\n",
    "#         with tf.io.TFRecordWriter(os.path.abspath(f\"{i // 2500}.tfrecord\")) as f:\n",
    "#             f.write(serialize_example(inst[0], inst[1]))\n",
    "#     os.chdir(\"../\")\n",
    "# os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32730f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92d8cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a78da17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.shuffle(10000).batch(batch_size).prefetch(1)\n",
    "# valid = valid.shuffle(10000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bd02d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_size = 100\n",
    "\n",
    "# encoder_input = keras.layers.Input(shape=[None])\n",
    "# encoder_embedding = keras.layers.Embedding(vocab_size + 1, embed_size, mask_zero=True, input_shape=[None])(encoder_input)\n",
    "# encoder_positional_embedding = keras.layers.Lambda(lambda x : x + tf.range(tf.shape(x)[1], dtype=tf.float32)[tf.newaxis, :, tf.newaxis])(encoder_embedding)\n",
    "# Z = encoder_positional_embedding\n",
    "# for _ in range(2):\n",
    "#     Z = keras.layers.Attention(use_scale=True)([Z,Z])\n",
    "# encoder_output = Z\n",
    "# encoder = keras.Model(inputs=[encoder_input], outputs=[encoder_output])\n",
    "\n",
    "# decoder_input = keras.layers.Input(shape=[None, embed_size])\n",
    "# gru1 = keras.layers.GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(decoder_input)\n",
    "# gru2 = keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2)(gru1)\n",
    "# decoder_output = keras.layers.Dense(1, activation=\"softmax\")(gru2)\n",
    "# decoder = keras.Model(inputs=[decoder_input], outputs=[decoder_output])\n",
    "\n",
    "# model = keras.models.Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67db59fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "# history=model.fit(train.take(10), epochs=5, validation_data=valid.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e1fa28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud command I used for training: \n",
    "# gcloud ai-platform jobs submit training stream_sentiments1 --region asia-southeast1 --scale-tier BASIC --runtime-version 2.11 --python-version 3.7 --package-path ./stream_sentiments_training --module-name stream_sentiments_training.task --staging-bucket gs://6727667276 --job-dir gs://6727667276 --project compute-387916\n",
    "\n",
    "# training/task.py script that I used\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--job-dir\", required=True, type=str)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# embed_size = 100\n",
    "# max_allowed_length = 200\n",
    "# vocab_size = 690956\n",
    "\n",
    "# distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "# with distribution.scope():\n",
    "\n",
    "#     class PositionalEmbedding(keras.layers.Layer):\n",
    "#         def __init__(self, max_time_steps, dtype=tf.float32):\n",
    "#             super().__init__(dtype=dtype)\n",
    "#             positional_embedding = tf.range(max_time_steps)[tf.newaxis, :, tf.newaxis]\n",
    "#             self.positional_embedding = tf.cast(positional_embedding, self.dtype)\n",
    "#         def call(self, inputs):\n",
    "#             shape = tf.shape(inputs)\n",
    "#             return inputs + self.positional_embedding[:, :shape[1], :]\n",
    "\n",
    "#     encoder_input = keras.layers.Input(shape=[None])\n",
    "#     encoder_embedding = keras.layers.Embedding(vocab_size + 1, embed_size, mask_zero=True, input_shape=[None])(encoder_input)\n",
    "#     encoder_positional_embedding = PositionalEmbedding(max_allowed_length)(encoder_embedding)\n",
    "#     Z = encoder_positional_embedding\n",
    "#     for _ in range(2):\n",
    "#         Z = keras.layers.Attention(use_scale=True)([Z,Z])\n",
    "#         encoder_output = Z\n",
    "#     encoder = keras.Model(inputs=[encoder_input], outputs=[encoder_output])\n",
    "\n",
    "#     decoder_input = keras.layers.Input(shape=[None, embed_size])\n",
    "#     gru1 = keras.layers.GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(decoder_input)\n",
    "#     gru2 = keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2)(gru1)\n",
    "#     decoder_output = keras.layers.Dense(1, activation=\"softmax\")(gru2)\n",
    "#     decoder = keras.Model(inputs=[decoder_input], outputs=[decoder_output])\n",
    "\n",
    "#     model = keras.models.Sequential([encoder, decoder])\n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# batch_size=40\n",
    "# tensorboard_callback = keras.callbacks.TensorBoard(\"gs://6727667276\")\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"gs://6727667276/training.1600000.processed.noemoticon.csv\", encoding='ansi')\n",
    "# df = df[['''@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D''', '0']]\n",
    "# df.columns = ['tweet', 'sentiment']\n",
    "# dataset_size = len(df.tweet)\n",
    "# train_size = int(dataset_size * 0.95)\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer(char_level=False, oov_token=0)\n",
    "# tokenizer.fit_on_texts(df.tweet)\n",
    "# vocab_size = len(tokenizer.word_index.keys())\n",
    "# tokenized = tokenizer.texts_to_sequences(df.tweet)\n",
    "# max_tweet_length = max(len(sequence) for sequence in tokenized)\n",
    "# X = df.to_numpy()[:, 0]\n",
    "# y = df.to_numpy()[:, 1]\n",
    "# X_train, y_train, X_valid, y_valid = X[:train_size], y[:train_size], X[train_size:], y[train_size:]\n",
    "# max_allowed_length = 200\n",
    "# def preprocess(tweets, sentiments):\n",
    "#     tokenized = tokenizer.texts_to_sequences(list(tweets)) # tokenize                                                                                                                  \n",
    "#     ragged_tensor = tf.ragged.constant(tokenized) # ragged tensor                                                                                                                      \n",
    "#     dense_tensor = ragged_tensor.to_tensor(default_value=-1) # apply_padding                                                                                                           \n",
    "#     if tf.shape(dense_tensor)[1] > max_allowed_length:\n",
    "#         dense_tensor = dense_tensor[:, :max_allowed_length] # truncate if needed                                                                                                       \n",
    "#     X = dense_tensor + 1 # shift padding token to index 0                                                                                                                              \n",
    "#     y = (sentiments / 4).astype(np.uint32)\n",
    "#     return tf.constant(X, dtype=tf.int32), tf.constant(y, dtype=tf.int32)\n",
    "# X_train_preprocessed, y_train_preprocessed = preprocess(X_train, y_train)\n",
    "# X_valid_preprocessed, y_valid_preprocessed = preprocess(X_valid, y_valid)\n",
    "# train = tf.data.Dataset.from_tensor_slices((X_train_preprocessed, y_train_preprocessed)).shuffle(10000).batch(batch_size).prefetch(1)\n",
    "# valid = tf.data.Dataset.from_tensor_slices((X_valid_preprocessed, y_valid_preprocessed)).shuffle(10000).batch(batch_size).prefetch(1)\n",
    "\n",
    "# history = mirrored_model.fit(train, epochs=5, validation_data = valid, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7136c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model.h5\") # save model\n",
    "\n",
    "# model = keras.models.load_model(\"model.h5\") # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b4096f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_version=\"0001\"\n",
    "# model_name = \"stream_sentiments_model\"\n",
    "# model_path = os.path.join(model_name, model_version)\n",
    "\n",
    "# tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f573b22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make smaller version of model\n",
    "\n",
    "# embed_size = 50\n",
    "\n",
    "# encoder_input = keras.layers.Input(shape=[None])\n",
    "# encoder_embedding = keras.layers.Embedding(vocab_size + 1, embed_size, mask_zero=True, input_shape=[None])(encoder_input)\n",
    "# encoder_positional_embedding = keras.layers.Lambda(lambda x : x + tf.range(tf.shape(x)[1], dtype=tf.float32)[tf.newaxis, :, tf.newaxis])(encoder_embedding)\n",
    "# Z = encoder_positional_embedding\n",
    "# for _ in range(2):\n",
    "#     Z = keras.layers.Attention(use_scale=True)([Z,Z])\n",
    "# encoder_output = Z\n",
    "# encoder = keras.Model(inputs=[encoder_input], outputs=[encoder_output])\n",
    "\n",
    "# decoder_input = keras.layers.Input(shape=[None, embed_size])\n",
    "# gru1 = keras.layers.GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(decoder_input)\n",
    "# gru2 = keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2)(gru1)\n",
    "# decoder_output = keras.layers.Dense(1, activation=\"softmax\")(gru2)\n",
    "# decoder = keras.Model(inputs=[decoder_input], outputs=[decoder_output])\n",
    "\n",
    "# model = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "# history=model.fit(train.take(10), epochs=5, validation_data=valid.take(5))\n",
    "\n",
    "# model_version=\"0002\"\n",
    "# model_name = \"stream_sentiments_model\"\n",
    "# model_path = os.path.join(model_name, model_version)\n",
    "\n",
    "# tf.saved_model.save(model, model_path)\n",
    "\n",
    "# model.save(\"small_model.h5\") # save model\n",
    "\n",
    "# model = keras.models.load_model(\"small_model.h5\") # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb484bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 2,\n",
       " 'to': 3,\n",
       " 'the': 4,\n",
       " 'a': 5,\n",
       " 'my': 6,\n",
       " 'and': 7,\n",
       " 'you': 8,\n",
       " 'is': 9,\n",
       " 'it': 10,\n",
       " 'in': 11,\n",
       " 'for': 12,\n",
       " 'of': 13,\n",
       " 'on': 14,\n",
       " 'me': 15,\n",
       " 'so': 16,\n",
       " 'have': 17,\n",
       " 'that': 18,\n",
       " 'but': 19,\n",
       " \"i'm\": 20,\n",
       " 'just': 21,\n",
       " 'with': 22,\n",
       " 'be': 23,\n",
       " 'at': 24,\n",
       " 'not': 25,\n",
       " 'was': 26,\n",
       " 'this': 27,\n",
       " 'now': 28,\n",
       " 'good': 29,\n",
       " 'up': 30,\n",
       " 'day': 31,\n",
       " 'out': 32,\n",
       " 'all': 33,\n",
       " 'get': 34,\n",
       " 'like': 35,\n",
       " 'are': 36,\n",
       " 'no': 37,\n",
       " 'go': 38,\n",
       " 'quot': 39,\n",
       " 'http': 40,\n",
       " 'today': 41,\n",
       " 'do': 42,\n",
       " \"it's\": 43,\n",
       " 'too': 44,\n",
       " 'your': 45,\n",
       " 'work': 46,\n",
       " 'love': 47,\n",
       " 'going': 48,\n",
       " 'got': 49,\n",
       " 'lol': 50,\n",
       " 'time': 51,\n",
       " 'back': 52,\n",
       " 'from': 53,\n",
       " 'u': 54,\n",
       " 'one': 55,\n",
       " 'what': 56,\n",
       " 'com': 57,\n",
       " 'will': 58,\n",
       " 'im': 59,\n",
       " 'know': 60,\n",
       " 'we': 61,\n",
       " 'about': 62,\n",
       " 'am': 63,\n",
       " 'really': 64,\n",
       " \"don't\": 65,\n",
       " 'amp': 66,\n",
       " 'had': 67,\n",
       " 'can': 68,\n",
       " 'see': 69,\n",
       " \"can't\": 70,\n",
       " 'some': 71,\n",
       " 'its': 72,\n",
       " 'if': 73,\n",
       " 'still': 74,\n",
       " '2': 75,\n",
       " 'well': 76,\n",
       " 'night': 77,\n",
       " 'new': 78,\n",
       " 'want': 79,\n",
       " 'how': 80,\n",
       " 'think': 81,\n",
       " 'home': 82,\n",
       " 'thanks': 83,\n",
       " 'oh': 84,\n",
       " 'when': 85,\n",
       " 'as': 86,\n",
       " 'there': 87,\n",
       " 'more': 88,\n",
       " 'miss': 89,\n",
       " 'off': 90,\n",
       " 'much': 91,\n",
       " 'here': 92,\n",
       " 'they': 93,\n",
       " 'need': 94,\n",
       " 'last': 95,\n",
       " 'an': 96,\n",
       " 'then': 97,\n",
       " 'morning': 98,\n",
       " 'hope': 99,\n",
       " 'tomorrow': 100,\n",
       " 'been': 101,\n",
       " 'great': 102,\n",
       " 'twitter': 103,\n",
       " 'has': 104,\n",
       " '3': 105,\n",
       " 'or': 106,\n",
       " 'haha': 107,\n",
       " 'her': 108,\n",
       " 'again': 109,\n",
       " 'feel': 110,\n",
       " 'sad': 111,\n",
       " 'he': 112,\n",
       " 'fun': 113,\n",
       " 'why': 114,\n",
       " 'wish': 115,\n",
       " 'only': 116,\n",
       " 'sleep': 117,\n",
       " 'right': 118,\n",
       " 'bad': 119,\n",
       " 'happy': 120,\n",
       " 'very': 121,\n",
       " 'would': 122,\n",
       " 'sorry': 123,\n",
       " 'tonight': 124,\n",
       " \"i'll\": 125,\n",
       " 'did': 126,\n",
       " 'by': 127,\n",
       " 'come': 128,\n",
       " 'make': 129,\n",
       " 'them': 130,\n",
       " 'way': 131,\n",
       " 'bit': 132,\n",
       " 'getting': 133,\n",
       " 'though': 134,\n",
       " 'gonna': 135,\n",
       " 'over': 136,\n",
       " 'nice': 137,\n",
       " 'better': 138,\n",
       " 'watching': 139,\n",
       " 'she': 140,\n",
       " 'yeah': 141,\n",
       " 'wait': 142,\n",
       " 'should': 143,\n",
       " 'bed': 144,\n",
       " \"that's\": 145,\n",
       " 'could': 146,\n",
       " 'week': 147,\n",
       " 'twitpic': 148,\n",
       " \"didn't\": 149,\n",
       " 'school': 150,\n",
       " 'people': 151,\n",
       " \"i've\": 152,\n",
       " 'hate': 153,\n",
       " 'him': 154,\n",
       " \"you're\": 155,\n",
       " 'days': 156,\n",
       " 'even': 157,\n",
       " 'hey': 158,\n",
       " 'after': 159,\n",
       " '4': 160,\n",
       " 'yes': 161,\n",
       " 'next': 162,\n",
       " 'lt': 163,\n",
       " 'down': 164,\n",
       " 'weekend': 165,\n",
       " 'awesome': 166,\n",
       " 'dont': 167,\n",
       " 'thank': 168,\n",
       " 'never': 169,\n",
       " 'soon': 170,\n",
       " 'were': 171,\n",
       " 'cant': 172,\n",
       " 'long': 173,\n",
       " 'take': 174,\n",
       " 'little': 175,\n",
       " 'working': 176,\n",
       " 'who': 177,\n",
       " 'first': 178,\n",
       " 'wanna': 179,\n",
       " 'please': 180,\n",
       " 'say': 181,\n",
       " 'best': 182,\n",
       " 'tired': 183,\n",
       " 'show': 184,\n",
       " 'ok': 185,\n",
       " 'doing': 186,\n",
       " 'having': 187,\n",
       " 'sick': 188,\n",
       " 'x': 189,\n",
       " 'being': 190,\n",
       " 'life': 191,\n",
       " 'everyone': 192,\n",
       " 'watch': 193,\n",
       " 'his': 194,\n",
       " 'done': 195,\n",
       " '1': 196,\n",
       " 'our': 197,\n",
       " 'any': 198,\n",
       " 'feeling': 199,\n",
       " 'always': 200,\n",
       " 'sure': 201,\n",
       " 'us': 202,\n",
       " 'thing': 203,\n",
       " 'already': 204,\n",
       " 'another': 205,\n",
       " 'cool': 206,\n",
       " 'friends': 207,\n",
       " 'than': 208,\n",
       " 'something': 209,\n",
       " 'find': 210,\n",
       " 'guys': 211,\n",
       " 'man': 212,\n",
       " 'ly': 213,\n",
       " 'ready': 214,\n",
       " 'made': 215,\n",
       " 'where': 216,\n",
       " 'yay': 217,\n",
       " 'because': 218,\n",
       " 'looking': 219,\n",
       " 'yet': 220,\n",
       " 'phone': 221,\n",
       " 'look': 222,\n",
       " 'went': 223,\n",
       " 'hours': 224,\n",
       " 'ur': 225,\n",
       " 'house': 226,\n",
       " 'before': 227,\n",
       " 'movie': 228,\n",
       " 'p': 229,\n",
       " 'ever': 230,\n",
       " 'tweet': 231,\n",
       " 'pretty': 232,\n",
       " 'maybe': 233,\n",
       " 'trying': 234,\n",
       " 'away': 235,\n",
       " 'omg': 236,\n",
       " 'summer': 237,\n",
       " 'old': 238,\n",
       " 'finally': 239,\n",
       " 'amazing': 240,\n",
       " 'early': 241,\n",
       " 'help': 242,\n",
       " '5': 243,\n",
       " 'let': 244,\n",
       " 'damn': 245,\n",
       " 'follow': 246,\n",
       " 'things': 247,\n",
       " 'guess': 248,\n",
       " 'lost': 249,\n",
       " 'left': 250,\n",
       " 'into': 251,\n",
       " 'keep': 252,\n",
       " 'big': 253,\n",
       " 'someone': 254,\n",
       " 'thought': 255,\n",
       " 'wow': 256,\n",
       " 'hot': 257,\n",
       " 'year': 258,\n",
       " 'missed': 259,\n",
       " 'rain': 260,\n",
       " 'same': 261,\n",
       " 'nothing': 262,\n",
       " 'sucks': 263,\n",
       " 'while': 264,\n",
       " 'girl': 265,\n",
       " 'friend': 266,\n",
       " 'n': 267,\n",
       " 'bored': 268,\n",
       " 'glad': 269,\n",
       " 'start': 270,\n",
       " 'try': 271,\n",
       " 'other': 272,\n",
       " 'live': 273,\n",
       " 'also': 274,\n",
       " 'looks': 275,\n",
       " \"won't\": 276,\n",
       " 'tell': 277,\n",
       " 'coming': 278,\n",
       " 'birthday': 279,\n",
       " \"doesn't\": 280,\n",
       " 'baby': 281,\n",
       " 'two': 282,\n",
       " 'does': 283,\n",
       " 'w': 284,\n",
       " 'later': 285,\n",
       " 'weather': 286,\n",
       " 'sun': 287,\n",
       " 'song': 288,\n",
       " 'ugh': 289,\n",
       " 'hear': 290,\n",
       " 'ya': 291,\n",
       " 'actually': 292,\n",
       " 'those': 293,\n",
       " 'stuff': 294,\n",
       " 'saw': 295,\n",
       " 'party': 296,\n",
       " 'makes': 297,\n",
       " 'excited': 298,\n",
       " 'waiting': 299,\n",
       " 'might': 300,\n",
       " 'hard': 301,\n",
       " 'play': 302,\n",
       " 'game': 303,\n",
       " 'said': 304,\n",
       " 'thats': 305,\n",
       " 'since': 306,\n",
       " 'until': 307,\n",
       " 'hi': 308,\n",
       " 'yesterday': 309,\n",
       " 'world': 310,\n",
       " 'god': 311,\n",
       " 'gotta': 312,\n",
       " 'late': 313,\n",
       " 'few': 314,\n",
       " 'such': 315,\n",
       " 'lot': 316,\n",
       " 'www': 317,\n",
       " 'myself': 318,\n",
       " 'around': 319,\n",
       " 'music': 320,\n",
       " 'o': 321,\n",
       " 'many': 322,\n",
       " 'car': 323,\n",
       " 'b': 324,\n",
       " 'sounds': 325,\n",
       " 'check': 326,\n",
       " 'found': 327,\n",
       " 'luck': 328,\n",
       " 'head': 329,\n",
       " 'friday': 330,\n",
       " 'r': 331,\n",
       " 'cold': 332,\n",
       " 'job': 333,\n",
       " 'must': 334,\n",
       " 'call': 335,\n",
       " 'read': 336,\n",
       " 'give': 337,\n",
       " 'beautiful': 338,\n",
       " 'their': 339,\n",
       " 'aww': 340,\n",
       " \"he's\": 341,\n",
       " 'making': 342,\n",
       " 'sunday': 343,\n",
       " \"haven't\": 344,\n",
       " 'may': 345,\n",
       " 'gone': 346,\n",
       " 'talk': 347,\n",
       " 'missing': 348,\n",
       " 'put': 349,\n",
       " 'mom': 350,\n",
       " 'poor': 351,\n",
       " 'anything': 352,\n",
       " 'stop': 353,\n",
       " 'monday': 354,\n",
       " 'least': 355,\n",
       " 'woke': 356,\n",
       " 'okay': 357,\n",
       " 'leave': 358,\n",
       " 'most': 359,\n",
       " 'almost': 360,\n",
       " 'cute': 361,\n",
       " 'free': 362,\n",
       " 'use': 363,\n",
       " 'times': 364,\n",
       " 'till': 365,\n",
       " 'lunch': 366,\n",
       " 'hair': 367,\n",
       " 'tho': 368,\n",
       " \"i'd\": 369,\n",
       " 'iphone': 370,\n",
       " 'far': 371,\n",
       " 'listening': 372,\n",
       " \"isn't\": 373,\n",
       " 'family': 374,\n",
       " 'hurts': 375,\n",
       " 'eat': 376,\n",
       " 'mean': 377,\n",
       " 'food': 378,\n",
       " 'end': 379,\n",
       " 'wanted': 380,\n",
       " 'funny': 381,\n",
       " 'dinner': 382,\n",
       " 'hour': 383,\n",
       " '10': 384,\n",
       " 'enjoy': 385,\n",
       " 'd': 386,\n",
       " 'gt': 387,\n",
       " 'finished': 388,\n",
       " 'shit': 389,\n",
       " 'playing': 390,\n",
       " 'sweet': 391,\n",
       " 'followers': 392,\n",
       " 'believe': 393,\n",
       " 'welcome': 394,\n",
       " 'anyone': 395,\n",
       " 'thinking': 396,\n",
       " 'everything': 397,\n",
       " 'forward': 398,\n",
       " 'without': 399,\n",
       " 'which': 400,\n",
       " '6': 401,\n",
       " 'video': 402,\n",
       " 'real': 403,\n",
       " 'hahaha': 404,\n",
       " 'totally': 405,\n",
       " 's': 406,\n",
       " 'outside': 407,\n",
       " 'tinyurl': 408,\n",
       " 'these': 409,\n",
       " 'coffee': 410,\n",
       " 'stupid': 411,\n",
       " 'enough': 412,\n",
       " 'buy': 413,\n",
       " 'mine': 414,\n",
       " 'ill': 415,\n",
       " 'wrong': 416,\n",
       " 'through': 417,\n",
       " 'anymore': 418,\n",
       " 'didnt': 419,\n",
       " \"we're\": 420,\n",
       " 'every': 421,\n",
       " 'probably': 422,\n",
       " 'tv': 423,\n",
       " 'xx': 424,\n",
       " 'room': 425,\n",
       " 'place': 426,\n",
       " 'ha': 427,\n",
       " 'weeks': 428,\n",
       " 'tweets': 429,\n",
       " 't': 430,\n",
       " 'once': 431,\n",
       " 'eating': 432,\n",
       " 'cause': 433,\n",
       " 'plurk': 434,\n",
       " 'money': 435,\n",
       " 'stay': 436,\n",
       " 'busy': 437,\n",
       " 'win': 438,\n",
       " 'sooo': 439,\n",
       " 'wants': 440,\n",
       " 'following': 441,\n",
       " \"she's\": 442,\n",
       " 'lovely': 443,\n",
       " '30': 444,\n",
       " 'saturday': 445,\n",
       " 'class': 446,\n",
       " 'pic': 447,\n",
       " '8': 448,\n",
       " 'came': 449,\n",
       " 'says': 450,\n",
       " 'whole': 451,\n",
       " 'seen': 452,\n",
       " 'kinda': 453,\n",
       " 'taking': 454,\n",
       " \"there's\": 455,\n",
       " '7': 456,\n",
       " 'crazy': 457,\n",
       " 'kids': 458,\n",
       " 'beach': 459,\n",
       " 'super': 460,\n",
       " 'hopefully': 461,\n",
       " 'exam': 462,\n",
       " 'news': 463,\n",
       " 'both': 464,\n",
       " 'headache': 465,\n",
       " 'took': 466,\n",
       " 'half': 467,\n",
       " 'guy': 468,\n",
       " 'hello': 469,\n",
       " 'name': 470,\n",
       " 'post': 471,\n",
       " 'true': 472,\n",
       " 'idea': 473,\n",
       " 'awww': 474,\n",
       " 'dad': 475,\n",
       " 'book': 476,\n",
       " 'face': 477,\n",
       " \"what's\": 478,\n",
       " 'years': 479,\n",
       " 'goodnight': 480,\n",
       " 'hell': 481,\n",
       " 'forgot': 482,\n",
       " 'able': 483,\n",
       " 'ago': 484,\n",
       " 'meet': 485,\n",
       " 'c': 486,\n",
       " 'either': 487,\n",
       " 'lots': 488,\n",
       " \"wasn't\": 489,\n",
       " 'reading': 490,\n",
       " 'full': 491,\n",
       " 'run': 492,\n",
       " 'leaving': 493,\n",
       " 'rest': 494,\n",
       " 'shopping': 495,\n",
       " 'sitting': 496,\n",
       " 'send': 497,\n",
       " 'else': 498,\n",
       " 'girls': 499,\n",
       " 'ah': 500,\n",
       " 'used': 501,\n",
       " 'boo': 502,\n",
       " 'soo': 503,\n",
       " 'computer': 504,\n",
       " 'feels': 505,\n",
       " 'hurt': 506,\n",
       " 'blog': 507,\n",
       " \"they're\": 508,\n",
       " 'seems': 509,\n",
       " 'alone': 510,\n",
       " 'raining': 511,\n",
       " \"couldn't\": 512,\n",
       " 'remember': 513,\n",
       " 'cuz': 514,\n",
       " 'talking': 515,\n",
       " 'heart': 516,\n",
       " 'fuck': 517,\n",
       " 'dog': 518,\n",
       " 'office': 519,\n",
       " 'btw': 520,\n",
       " 'hehe': 521,\n",
       " 'hit': 522,\n",
       " 'own': 523,\n",
       " 'watched': 524,\n",
       " 'tried': 525,\n",
       " 'stuck': 526,\n",
       " 'trip': 527,\n",
       " 'needs': 528,\n",
       " 'heard': 529,\n",
       " 'm': 530,\n",
       " 'mind': 531,\n",
       " 'boy': 532,\n",
       " 'started': 533,\n",
       " 'kind': 534,\n",
       " 'internet': 535,\n",
       " 'course': 536,\n",
       " 'facebook': 537,\n",
       " 'part': 538,\n",
       " 'seeing': 539,\n",
       " 'wont': 540,\n",
       " 'online': 541,\n",
       " 'mileycyrus': 542,\n",
       " 'using': 543,\n",
       " 'quite': 544,\n",
       " 'picture': 545,\n",
       " 'awake': 546,\n",
       " 'break': 547,\n",
       " 'pain': 548,\n",
       " 'add': 549,\n",
       " 'fine': 550,\n",
       " 'breakfast': 551,\n",
       " 'cry': 552,\n",
       " 'pics': 553,\n",
       " 'loved': 554,\n",
       " 'told': 555,\n",
       " 'change': 556,\n",
       " 'goes': 557,\n",
       " 'boring': 558,\n",
       " 'seriously': 559,\n",
       " 'sunny': 560,\n",
       " 'wake': 561,\n",
       " 'person': 562,\n",
       " 'la': 563,\n",
       " 'lmao': 564,\n",
       " 'bought': 565,\n",
       " 'update': 566,\n",
       " 'broke': 567,\n",
       " 'care': 568,\n",
       " 'ass': 569,\n",
       " 'lucky': 570,\n",
       " 'dude': 571,\n",
       " 'open': 572,\n",
       " 'minutes': 573,\n",
       " 'concert': 574,\n",
       " 'gets': 575,\n",
       " 'called': 576,\n",
       " 'starting': 577,\n",
       " 'june': 578,\n",
       " 'season': 579,\n",
       " 'reply': 580,\n",
       " 'asleep': 581,\n",
       " 'hungry': 582,\n",
       " 'anyway': 583,\n",
       " 'aw': 584,\n",
       " 'pay': 585,\n",
       " '9': 586,\n",
       " 'e': 587,\n",
       " 'fan': 588,\n",
       " 'crap': 589,\n",
       " 'link': 590,\n",
       " 'site': 591,\n",
       " 'bring': 592,\n",
       " 'afternoon': 593,\n",
       " 'month': 594,\n",
       " \"you'll\": 595,\n",
       " 'favorite': 596,\n",
       " 'walk': 597,\n",
       " 'drive': 598,\n",
       " 'shower': 599,\n",
       " 'red': 600,\n",
       " 'sleeping': 601,\n",
       " 'til': 602,\n",
       " 'ice': 603,\n",
       " 'heading': 604,\n",
       " 'instead': 605,\n",
       " 'study': 606,\n",
       " 'xd': 607,\n",
       " '100': 608,\n",
       " 'jealous': 609,\n",
       " 'bye': 610,\n",
       " 'train': 611,\n",
       " 'text': 612,\n",
       " 'yea': 613,\n",
       " 'youtube': 614,\n",
       " 'exams': 615,\n",
       " 'enjoying': 616,\n",
       " 'rock': 617,\n",
       " 'mad': 618,\n",
       " 'tommcfly': 619,\n",
       " 'wonderful': 620,\n",
       " 'hoping': 621,\n",
       " '0': 622,\n",
       " 'high': 623,\n",
       " 'move': 624,\n",
       " 'sore': 625,\n",
       " 'definitely': 626,\n",
       " 'soooo': 627,\n",
       " 'together': 628,\n",
       " 'city': 629,\n",
       " 'running': 630,\n",
       " 'problem': 631,\n",
       " 'dead': 632,\n",
       " 'sister': 633,\n",
       " 'homework': 634,\n",
       " 'sometimes': 635,\n",
       " 'congrats': 636,\n",
       " 'fail': 637,\n",
       " '12': 638,\n",
       " 'finish': 639,\n",
       " 'died': 640,\n",
       " 'ask': 641,\n",
       " 'means': 642,\n",
       " 'fm': 643,\n",
       " 'fucking': 644,\n",
       " 'happened': 645,\n",
       " 'write': 646,\n",
       " 'dear': 647,\n",
       " 'works': 648,\n",
       " 'bout': 649,\n",
       " 'movies': 650,\n",
       " 'email': 651,\n",
       " 'won': 652,\n",
       " 'album': 653,\n",
       " 'sigh': 654,\n",
       " 'suck': 655,\n",
       " 'drink': 656,\n",
       " 'town': 657,\n",
       " 'couple': 658,\n",
       " \"we'll\": 659,\n",
       " 'top': 660,\n",
       " 'laptop': 661,\n",
       " 'boys': 662,\n",
       " 'cut': 663,\n",
       " 'loves': 664,\n",
       " 'brother': 665,\n",
       " 'church': 666,\n",
       " 'eyes': 667,\n",
       " 'set': 668,\n",
       " 'nite': 669,\n",
       " 'comes': 670,\n",
       " 'evening': 671,\n",
       " 'tour': 672,\n",
       " 'tea': 673,\n",
       " 'less': 674,\n",
       " 'ppl': 675,\n",
       " 'dream': 676,\n",
       " 'goin': 677,\n",
       " 'perfect': 678,\n",
       " 'months': 679,\n",
       " 'water': 680,\n",
       " 'ddlovato': 681,\n",
       " 'reason': 682,\n",
       " 'ipod': 683,\n",
       " 'weird': 684,\n",
       " 're': 685,\n",
       " 'ive': 686,\n",
       " 'happen': 687,\n",
       " 'nap': 688,\n",
       " 'cream': 689,\n",
       " 'studying': 690,\n",
       " 'final': 691,\n",
       " 'sound': 692,\n",
       " 'side': 693,\n",
       " '20': 694,\n",
       " 'gym': 695,\n",
       " 'visit': 696,\n",
       " 'songs': 697,\n",
       " 'meeting': 698,\n",
       " 'dance': 699,\n",
       " 'listen': 700,\n",
       " 'test': 701,\n",
       " 'close': 702,\n",
       " 'ones': 703,\n",
       " 'cat': 704,\n",
       " 'fall': 705,\n",
       " 'loving': 706,\n",
       " 'uk': 707,\n",
       " 'english': 708,\n",
       " 'interesting': 709,\n",
       " 'seem': 710,\n",
       " 'tickets': 711,\n",
       " 'fb': 712,\n",
       " 'mood': 713,\n",
       " 'y': 714,\n",
       " 'blip': 715,\n",
       " 'store': 716,\n",
       " 'hang': 717,\n",
       " 'story': 718,\n",
       " 'agree': 719,\n",
       " \"let's\": 720,\n",
       " 'catch': 721,\n",
       " 'clean': 722,\n",
       " 'lil': 723,\n",
       " 'ate': 724,\n",
       " 'list': 725,\n",
       " '11': 726,\n",
       " 'myspace': 727,\n",
       " 'moment': 728,\n",
       " \"'\": 729,\n",
       " 'ahh': 730,\n",
       " 'second': 731,\n",
       " 'writing': 732,\n",
       " 'mr': 733,\n",
       " 'knew': 734,\n",
       " 'short': 735,\n",
       " 'turn': 736,\n",
       " 'fast': 737,\n",
       " 'word': 738,\n",
       " 'pool': 739,\n",
       " 'worst': 740,\n",
       " 'broken': 741,\n",
       " 'air': 742,\n",
       " 'chocolate': 743,\n",
       " 'awards': 744,\n",
       " 'london': 745,\n",
       " 'l': 746,\n",
       " 'smile': 747,\n",
       " 'ride': 748,\n",
       " 'page': 749,\n",
       " 'wishing': 750,\n",
       " 'saying': 751,\n",
       " 'hmm': 752,\n",
       " 'unfortunately': 753,\n",
       " 'black': 754,\n",
       " 'yep': 755,\n",
       " 'worth': 756,\n",
       " 'xxx': 757,\n",
       " 'k': 758,\n",
       " 'supposed': 759,\n",
       " 'moving': 760,\n",
       " 'three': 761,\n",
       " 'star': 762,\n",
       " 'da': 763,\n",
       " 'via': 764,\n",
       " '1st': 765,\n",
       " 'driving': 766,\n",
       " 'sunshine': 767,\n",
       " 'jonas': 768,\n",
       " 'throat': 769,\n",
       " 'past': 770,\n",
       " 'sleepy': 771,\n",
       " 'cleaning': 772,\n",
       " 'lady': 773,\n",
       " 'photo': 774,\n",
       " 'wedding': 775,\n",
       " 'dreams': 776,\n",
       " 'pictures': 777,\n",
       " 'forget': 778,\n",
       " 'park': 779,\n",
       " 'sent': 780,\n",
       " 'horrible': 781,\n",
       " 'gave': 782,\n",
       " 'understand': 783,\n",
       " 'followfriday': 784,\n",
       " 'tweeting': 785,\n",
       " 'wonder': 786,\n",
       " 'drinking': 787,\n",
       " 'college': 788,\n",
       " 'green': 789,\n",
       " 'pick': 790,\n",
       " 'plan': 791,\n",
       " 'team': 792,\n",
       " 'account': 793,\n",
       " 'mac': 794,\n",
       " '15': 795,\n",
       " 'chance': 796,\n",
       " 'whats': 797,\n",
       " 'slow': 798,\n",
       " 'hugs': 799,\n",
       " \"wouldn't\": 800,\n",
       " 'worse': 801,\n",
       " 'sat': 802,\n",
       " 'under': 803,\n",
       " 'date': 804,\n",
       " 'easy': 805,\n",
       " 'moon': 806,\n",
       " 'doesnt': 807,\n",
       " 'rather': 808,\n",
       " 'longer': 809,\n",
       " 'apparently': 810,\n",
       " 'fell': 811,\n",
       " \"aren't\": 812,\n",
       " 'scared': 813,\n",
       " 'mum': 814,\n",
       " 'vote': 815,\n",
       " 'bet': 816,\n",
       " 'cannot': 817,\n",
       " 'special': 818,\n",
       " 'upset': 819,\n",
       " 'hand': 820,\n",
       " 'point': 821,\n",
       " 'flight': 822,\n",
       " 'wtf': 823,\n",
       " 'flu': 824,\n",
       " 'bday': 825,\n",
       " 'holiday': 826,\n",
       " 'updates': 827,\n",
       " 'nope': 828,\n",
       " 'spent': 829,\n",
       " 'david': 830,\n",
       " 'mtv': 831,\n",
       " 'due': 832,\n",
       " 'tuesday': 833,\n",
       " 'huge': 834,\n",
       " 'plus': 835,\n",
       " 'band': 836,\n",
       " 'plans': 837,\n",
       " 'miley': 838,\n",
       " 'thx': 839,\n",
       " 'earlier': 840,\n",
       " 'fair': 841,\n",
       " 'white': 842,\n",
       " 'hanging': 843,\n",
       " 'spend': 844,\n",
       " 'ahhh': 845,\n",
       " 'words': 846,\n",
       " 'parents': 847,\n",
       " 'wondering': 848,\n",
       " \"you've\": 849,\n",
       " 'forever': 850,\n",
       " 'shame': 851,\n",
       " 'body': 852,\n",
       " 'bus': 853,\n",
       " 'vacation': 854,\n",
       " 'lazy': 855,\n",
       " 'join': 856,\n",
       " 'website': 857,\n",
       " 'idk': 858,\n",
       " 'worry': 859,\n",
       " 'line': 860,\n",
       " 'message': 861,\n",
       " 'especially': 862,\n",
       " 'slept': 863,\n",
       " 'shows': 864,\n",
       " 'beer': 865,\n",
       " 'during': 866,\n",
       " 'photos': 867,\n",
       " 'lets': 868,\n",
       " 'wear': 869,\n",
       " 'answer': 870,\n",
       " 'voice': 871,\n",
       " 'google': 872,\n",
       " 'sadly': 873,\n",
       " 'warm': 874,\n",
       " 'stomach': 875,\n",
       " 'fans': 876,\n",
       " 'july': 877,\n",
       " 'thursday': 878,\n",
       " 'support': 879,\n",
       " 'thinks': 880,\n",
       " 'cake': 881,\n",
       " 'die': 882,\n",
       " 'tom': 883,\n",
       " 'learn': 884,\n",
       " 'looked': 885,\n",
       " 'chat': 886,\n",
       " 'figure': 887,\n",
       " 'different': 888,\n",
       " 'jonasbrothers': 889,\n",
       " 'havent': 890,\n",
       " 'inside': 891,\n",
       " 'blue': 892,\n",
       " 'yummy': 893,\n",
       " 'luv': 894,\n",
       " 'number': 895,\n",
       " 'meant': 896,\n",
       " 'sims': 897,\n",
       " 'airport': 898,\n",
       " 'met': 899,\n",
       " 'radio': 900,\n",
       " 'fix': 901,\n",
       " 'v': 902,\n",
       " 'safe': 903,\n",
       " 'rainy': 904,\n",
       " 'pizza': 905,\n",
       " 'crying': 906,\n",
       " 'camera': 907,\n",
       " 'liked': 908,\n",
       " 'episode': 909,\n",
       " 'eye': 910,\n",
       " 'small': 911,\n",
       " 'dress': 912,\n",
       " 'officially': 913,\n",
       " 'paper': 914,\n",
       " 'shop': 915,\n",
       " 'bbq': 916,\n",
       " 'except': 917,\n",
       " 'shirt': 918,\n",
       " '2day': 919,\n",
       " 'graduation': 920,\n",
       " 'boyfriend': 921,\n",
       " 'power': 922,\n",
       " 'apple': 923,\n",
       " 'games': 924,\n",
       " 'finals': 925,\n",
       " 'absolutely': 926,\n",
       " 'shoes': 927,\n",
       " 'kill': 928,\n",
       " 'project': 929,\n",
       " 'tummy': 930,\n",
       " 'laugh': 931,\n",
       " 'garden': 932,\n",
       " 'worked': 933,\n",
       " 'felt': 934,\n",
       " 'shall': 935,\n",
       " 'save': 936,\n",
       " \"it'll\": 937,\n",
       " 'ff': 938,\n",
       " 'twilight': 939,\n",
       " 'proud': 940,\n",
       " 'decided': 941,\n",
       " 'beat': 942,\n",
       " 'yourself': 943,\n",
       " 'alright': 944,\n",
       " 'chicken': 945,\n",
       " 'bike': 946,\n",
       " 'each': 947,\n",
       " 'needed': 948,\n",
       " 'road': 949,\n",
       " 'j': 950,\n",
       " 'kid': 951,\n",
       " 'lonely': 952,\n",
       " 'xoxo': 953,\n",
       " 'gorgeous': 954,\n",
       " 'yup': 955,\n",
       " 'books': 956,\n",
       " 'hug': 957,\n",
       " 'hospital': 958,\n",
       " 'club': 959,\n",
       " 'annoying': 960,\n",
       " 'played': 961,\n",
       " 'brothers': 962,\n",
       " 'woo': 963,\n",
       " 'wine': 964,\n",
       " 'case': 965,\n",
       " 'exciting': 966,\n",
       " 'sign': 967,\n",
       " 'pink': 968,\n",
       " 'wit': 969,\n",
       " 'exactly': 970,\n",
       " 'hmmm': 971,\n",
       " 'starts': 972,\n",
       " 'card': 973,\n",
       " 'wishes': 974,\n",
       " 'lame': 975,\n",
       " 'son': 976,\n",
       " 'keeps': 977,\n",
       " 'babe': 978,\n",
       " 'living': 979,\n",
       " 'front': 980,\n",
       " 'french': 981,\n",
       " 'feet': 982,\n",
       " 'taken': 983,\n",
       " 'ouch': 984,\n",
       " 'cd': 985,\n",
       " 'wednesday': 986,\n",
       " 'packing': 987,\n",
       " 'hubby': 988,\n",
       " 'f': 989,\n",
       " 'dm': 990,\n",
       " 'scary': 991,\n",
       " 'hates': 992,\n",
       " 'near': 993,\n",
       " 'fact': 994,\n",
       " 'goodbye': 995,\n",
       " 'business': 996,\n",
       " 'turned': 997,\n",
       " 'share': 998,\n",
       " 'happens': 999,\n",
       " 'yo': 1000,\n",
       " 'question': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make even smaller version of model, that includes preprocessing\n",
    "\n",
    "# start by deleting this useless word index from our existing tokenizer\n",
    "del tokenizer.word_index[0]\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aedf3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "# encoder_input = [\"Hello! I'm here! \", \"What? Where are you?\"]\n",
    "\n",
    "# split\n",
    "regex = \"\"\"'|!|\"|#|\\$|%|&|\\(|\\)|\\*|\\+|,|-|\\.|/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|\\^|_|`|\\{|\\||\\}|~|\\t|\\n\"\"\"\n",
    "split = keras.layers.Lambda(lambda x : tf.strings.split(\n",
    "                                       tf.strings.lower(\n",
    "                                       tf.strings.regex_replace(x, regex, \"\")\n",
    "                                       )))(encoder_input)\n",
    "\n",
    "# tokenize\n",
    "class VocabLookup(keras.layers.Layer):\n",
    "    def __init__(self, word_index, num_oov_buckets, **kwargs):\n",
    "        self.word_index = word_index\n",
    "        self.vocab = list(word_index.keys())\n",
    "        self.indices = tf.range(len(self.vocab), dtype=tf.int64)\n",
    "        table_init = tf.lookup.KeyValueTensorInitializer(self.vocab, self.indices)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
    "        super(VocabLookup, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.table.lookup(X)\n",
    "num_oov_buckets = 3\n",
    "tokenize = VocabLookup(tokenizer.word_index, num_oov_buckets)(split)\n",
    "\n",
    "# pad\n",
    "pad = keras.layers.Lambda(lambda x : x.to_tensor(default_value=-1)\n",
    "                                        + tf.constant(1, dtype=tf.int64))(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d6b76da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embed\n",
    "embed_size = 2\n",
    "vocab_size = len(tokenizer.word_index.keys())\n",
    "encoder_embedding = keras.layers.Embedding(vocab_size + num_oov_buckets + 1, embed_size, \n",
    "                                           mask_zero=True)(pad)\n",
    "\n",
    "# positional embed\n",
    "encoder_positional_embedding = keras.layers.Lambda(lambda x : x + tf.range(tf.shape(x)[1], \n",
    "                                dtype=tf.float32)[tf.newaxis, :, tf.newaxis])(encoder_embedding)\n",
    "\n",
    "# attention\n",
    "Z = encoder_positional_embedding\n",
    "for _ in range(2):\n",
    "    Z = keras.layers.Attention(use_scale=True, dropout=0.2)([Z,Z])\n",
    "encoder_output = Z\n",
    "\n",
    "# encoder\n",
    "encoder = keras.Model(inputs=[encoder_input], outputs=[encoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85b4e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = keras.layers.Input(shape=[None, embed_size])\n",
    "\n",
    "gru1 = keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(decoder_input)\n",
    "gru2 = keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2)(gru1)\n",
    "decoder_output = keras.layers.Dense(1, activation=\"sigmoid\")(gru2)\n",
    "decoder = keras.Model(inputs=[decoder_input], outputs=[decoder_output])\n",
    "\n",
    "# decoder\n",
    "model = keras.models.Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31c30783",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_2 (Functional)        (None, None, 2)           1381920   \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, 1)                 66273     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,448,193\n",
      "Trainable params: 1,448,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f47da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "batch_size = 20\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((X_train, \n",
    "                                            (y_train/4).astype(np.uint32)\n",
    "                                           ))\n",
    "train = train.shuffle(10000).batch(batch_size).prefetch(1)\n",
    "\n",
    "valid = tf.data.Dataset.from_tensor_slices((X_valid, \n",
    "                                            (y_valid/4).astype(np.uint32)\n",
    "                                           ))\n",
    "valid = valid.shuffle(10000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0564581",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_set = train.take(100)\n",
    "valid_set = valid.take(10)\n",
    "\n",
    "epochs = 1\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33bce4b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 271s 2s/step - loss: 0.7026 - accuracy: 0.5070 - val_loss: 0.7010 - val_accuracy: 0.4600\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_set, epochs=epochs, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3a0ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: stream_sentiments_model\\0002\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: stream_sentiments_model\\0002\\assets\n"
     ]
    }
   ],
   "source": [
    "model_version=\"0002\"\n",
    "\n",
    "model_name = \"stream_sentiments_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e429e79e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: 'VocabLookup'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meven_smaller_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meven_smaller_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# load model\u001b[39;00m\n",
      "File \u001b[1;32m~\\ml\\StreamSentiments\\stream_sentiments_env2\\lib\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    213\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    214\u001b[0m )\n",
      "File \u001b[1;32m~\\ml\\StreamSentiments\\stream_sentiments_env2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\ml\\StreamSentiments\\stream_sentiments_env2\\lib\\site-packages\\keras\\saving\\legacy\\serialization.py:368\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mget_registered_object(\n\u001b[0;32m    365\u001b[0m     class_name, custom_objects, module_objects\n\u001b[0;32m    366\u001b[0m )\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand that this object is included in the scope. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    376\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown layer: 'VocabLookup'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "model.save(\"even_smaller_model.h5\") # save model\n",
    "\n",
    "model = keras.models.load_model(\"even_smaller_model.h5\") # load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39742b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57973665"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the model\n",
    "\n",
    "message = \"Pleased to meet you\"\n",
    "\n",
    "model(tf.constant([message])).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e954834f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# docker run -it --rm -p 8500:8500 -p 8501:8501 -v \"C:\\\\Users\\\\vaheg\\\\ml\\\\StreamSentiments\\\\stream-sentiments\\\\stream_sentiments_model:/models/stream_sentiments_model\" -e MODEL_NAME=stream_sentiments_model tensorflow/serving \n",
    "# import numpy as np\n",
    "# X_new = np.random.randint(vocab_size, size=(10, 10)) #######################\n",
    "# import json \n",
    "# input_data_json = json.dumps({\n",
    "#     \"signature_name\" :  \"serving_default\",\n",
    "#     \"instances\" :  X_new.tolist()\n",
    "# })\n",
    "# import requests\n",
    "# server_url = \"http://localhost:8501/v1/models/stream_sentiments_model:predict\" ###############\n",
    "# response = requests.post(server_url, data=input_data_json)\n",
    "# response.raise_for_status()\n",
    "# response = response.json()\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37077e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "\n",
    "# def add_preprocessing():\n",
    "#     small_model_with_preprocessing = keras.models.Sequential()\n",
    "#     model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "#     for layer in range(n_hidden):\n",
    "#         model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "#     model.add(keras.layers.Dense(1))\n",
    "#     optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "#     model.compile(loss='mse', optimizer='adam')\n",
    "#     return model\n",
    "\n",
    "# make regressor that will be compatible with scikit-learn\n",
    "# keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3804976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "regex = \"\"\"'|!|\"|#|\\$|%|&|\\(|\\)|\\*|\\+|,|-|\\.|/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|\\^|_|`|\\{|\\||\\}|~|\\t|\\n\"\"\"\n",
    "class VocabLookup(keras.layers.Layer):\n",
    "    def __init__(self, word_index, num_oov_buckets, **kwargs):\n",
    "        self.word_index = word_index\n",
    "        self.vocab = list(word_index.keys())\n",
    "        self.indices = tf.range(len(self.vocab), dtype=tf.int64)\n",
    "        table_init = tf.lookup.KeyValueTensorInitializer(self.vocab, self.indices)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
    "        super(VocabLookup, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.table.lookup(X)\n",
    "model = keras.models.load_model(\"even_smaller_model.h5\", custom_objects={\"VocabLookup\":VocabLookup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f96de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
